# Análisis del paper Evaluating Recommendation Systems
Autores: Guy Shani, Asela Gunawardana

## Breve Resumen
<p align="justify">
  La meta de este paper es entregar información sobre cómo evaluar distintos sistemas recomendadores. Para ello, primero se explica los tipos de experimentos que se puede realizar: experimentos offline, que utilizan data existente de usuarios para simular el comportamiento de usuarios reales; experimentos con grupos de control, que son usuarios potenciales bajo supervisión; y experimentos online, con usuarios reales en un ambiente real. Además de describir estos tipos de experimentos, se mencionan algunas de las ventajas y desventajas de cada uno, así como las consideraciones a tener en mente. Luego, se describen los métodos de medición que pueden ser usados para comparar los sistemas evaluados: p-value, sign testing para experiencias que han sido probadas en todos los algoritmos (o sus alternativas más completas: paired Student’s t-test y Wilcoxon signed rank test), Mann-Whitney test (extensión del Wilcoxon signed rank test para las observaciones que no fueron hechas por el mismo usuario en todos los algoritmos) e intervalos de confianza. Finalmente, se realiza el análisis de algunas características de interés de los sistemas recomendadores y se entregan posibilidades de cómo medir qué alternativa es mejor según cada característica. Por ejemplo, se mencionan distintos tipos de precisión y cómo abordar las mediciones según el objetivo que se quiera optimizar al escoger un sistema recomendador. Entre ellos, se considera la precisión de ratings, que ya ha sido mencionada en papers de semanas anteriores, y las formas de comparar algoritmos basados en esta propiedad, como el RMSE y al MAE.
</p>

## Crítica personal
<p align="justify">
  En primer lugar, me parece destacable el enfoque que se da a este paper respecto a los sistemas recomendadores. Como se expresa tanto en este paper como en *Collaborative Filtering Recommender Systems* (J. Ben Schafer, Dan Frankowski, Jon Herlocker, Shilad Sen), los sistemas recomendadores han tenido un enorme desarrollo en el último tiempo. En consecuencia, la cantidad de información disponible sobre RecSys y los algoritmos y modelos existentes puede ser incluso intimidante para quienes quieren comenzar a usarlos y no tienen mucho conocimiento al respecto. En ese sentido, este paper de alguna forma pasa a ser un recomendador de sistemas recomendadores, pues sirve como guía para saber qué evaluar sobre un algoritmo según las necesidades del problema a resolver.
</p>

<p align="justify">
  En segundo lugar, me parece muy útil entender los tipos de experimentos que se recomienda hacer para evaluar sistemas recomendadores, esto es, los experimentos offline, con grupos de control y online. Efectivamente, como se indica en el paper, todos tienen sus ventajas y desventajas, las que deben ser bien entendidas y aprovechadas en base a que los recursos son limitados. Un punto particular que se menciona es que, para los grupos de control, lo ideal es que los sujetos de prueba no conozcan a priori la finalidad del experimento. No obstante, creo que al evaluar sistemas recomendadores se puede hacer muy difícil ocultar lo que se busca. Por ejemplo, si me pusieran frente a una interfaz y me pidieran elegir películas o música dentro de un conjunto de ítems una y otra vez, la primera intuición por mi parte sería pensar que están tratando de evaluar si las recomendaciones fueron bien predichas.
</p>

<p align="justify">
  En tercer lugar, me surgen algunas dudas sobre cómo se plantean los distintos tipos de precisión. Según entendí, la diferencia entre la predicción de ratings, la predicción de uso y la predicción de rankings es que la primera busca predecir el rating que le daría un usuario a un ítem, la segunda busca solo predecir si el usuario estaría interesado en consumir un ítem y la tercera busca ordenar una serie de ítems según cuál sería más interesante para el usuario. En este sentido, ¿no se necesitaría una especie de ranking en los tres casos? Para predecir los ítems con más probabilidad de ser usados, se debería cuantificar de alguna forma la afinidad item-usuario y entregar los ítems que sean más afines (ranking por afinidad), que a su vez podría perfectamente traducirse en un rating. Incluso, si se tiene en consideración la idea de que ciertos usuarios suelen ser más positivos con los ratings que otros, se podría usar la información de afinidad de usuario para tomar en cuenta esta consideración. Así, los ratings basados en afinidad se calcularían según dónde se ubique la media de sus afinidades. Con esto dicho, me parece que hacer la distinción entre los tres tipos de predicción no se hace 100% evidente. A mi parecer, se tiene que los tres tipos de recomendación están basados en la predicción de un ranking, solo que la predicción de uso añade algo al ranking (muestra solo los K mejores dado un threshold) y la predicción de rating añade algo más a la predicción de uso (entrega una calificación numérica).
</p>

<p align="justify">
  En cuarto lugar, me parece interesante el uso del índice de Gini para evaluar sistemas recomendadores, pues hasta ahora solo lo había escuchado como medida de desigualdad entre países. Su uso tiene mucho sentido, pues se podría hacer la analogía tal que los ítems son países y las interacciones de usuarios con ellos son los recursos. Mientras más interacciones (recursos) tenga in ítem (país), más rico es este. Así, mientras más diferencia haya entre las interacciones (recursos), más desigualdad hay. En la misma línea, también me parece destacable ese trade-off entre coverage y precisión. Aquí se podría quizá pensar en el principio de Pareto del 80/20. Si el 80% de los ingresos que genere la aplicación de un sistema recomendador es entregado por el 20% de los usuarios, ¿valdrá la pena tratar de mejorar el coverage con el riesgo de disminuir la calidad de servicio entregado a ese 20% de usuarios? Esto es, efectivamente, algo que no puede ser ignorado al momento de elegir un sistema recomendador en un área de uso comercial.
</p>

<p align="justify">
  En quinto lugar, creo que es de utilidad mencionar atributos de los sistemas recomendadores que son más complicados de medir, pero que son importantes para los resultados. Por ejemplo, la confianza del usuario por las recomendaciones del sistema parece ser un tema crucial, sobre todo en ámbitos comerciales: si el usuario no confía o deja de confiar en las recomendaciones, ¿por qué usaría el sistema? Esto puede relacionarse con el riesgo asociado a las pruebas online que se menciona en el paper, que si bien pueden ser gran fuente de información, hacer un test online con un sistema que falle puede hacer que los usuarios no quieran usarlo más. Otro de estos factores complejos de medir es la utilidad de un sistema recomendador, puesto que este depende totalmente de la organización que decida implementar el sistema recomendador. Es importante medir esto, principalmente en el sector privado, pues la inversión de implementar un sistema recomendador podría no ser menor. Si bien es cierto que no toda la investigación en el campo se realiza con fines monetarios, también es cierto que el sector privado permite el desarrollo de nuevas tecnologías gracias a la disposición de fondos. Un caso claro de esta situación es el Netflix Prize y sus implicancias en el desarrollo de los sistemas recomendadores.
</p>

<p align="justify">
  Finalmente, algo que siento que faltó en este paper es hacer una comparativa real. Durante una buena parte de la lectura creí que en algún momento se pasaría de la teoría a la práctica y se testearía una serie de sistemas recomendadores en base a los procedimientos y métricas que se estaban describiendo, y que de esta forma se llegaría a indicar qué algoritmos habían funcionado mejor bajo una serie de escenarios y condiciones dadas. No obstante, imagino que la realización de tal experimento podría ser muy costosa e incluso demasiado amplia como para tratarla en un mismo paper.
</p>
